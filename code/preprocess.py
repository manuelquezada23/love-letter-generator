import tensorflow as tf
import numpy as np
import pandas as pd
import nltk
from functools import reduce
from sklearn.utils import shuffle 
import os
import re
import pickle
import glob

"""
reads poem 
"""
def read_poems(data_path):
    # https://stackoverflow.com/questions/7099290/how-to-ignore-hidden-files-using-os-listdir
    poems = glob.glob(os.path.join(data_path, '*'))
    print('Processing:', len(poems), 'files\n')

    poem_data = []
    purge = 0
    for poem in poems:
        poem_file_path = poem
        with open(poem_file_path, "r", encoding='utf-8') as poem_file:
            file = poem_file.read()
            file = file.encode('utf-8', errors='ignore').decode('utf-8')
            file = file.lower()
            file = re.sub(' +', ' ', file).strip()
            chars_remove = ['\t', '"', '%', '&', '\*', '\+', '/', '0', '1', '2', '3', '4', 'Â¦',
                            '5', '6', '7', '8', '9', '=', '\[', '\]', '_',  '`', '{',
                            'Â¨', 'Âª', 'Â«', 'Â²', 'Âº', 'Â»', 'Ã ', 'Ã¢', 'Ã£', 'Ã¤', 'Ã§',
                            'Ã¨', 'Ãª', 'Ã¬', 'Ã¯', 'Ã²', 'Ã´', 'Ãµ', 'Ã¶', 'Ã¼', '\xa0', 'Â°', 
                            '~','Â¤','Â©','Â®','Â·','Ã¡','Ã¦','Ã©','Ã«','Ã­','Ã®','Ã°','Ã±','Ã³','Ã¸','Ã¹','Ãº','Ã»','ä¾„', 'Ã½','Ã¾','Ä','Äƒ','Ä‡','Ä','Ä‘','Ä«','Å“','Å›','ÅŸ','Å¡','Å£','Å©','Å«','Å¾','Æ¡','Æ°','È™','È›Ì€ÌÌƒÌ‰Ì£','Î¬','Î­','Î®','Î¯','Î±','Î²','Î³','Î´','Îµ','Î¶','Î·','Î¸','Î¹','Îº','Î»','Î¼','Î½','Î¾','Î¿','Ï€','Ï','Ï‚','Ïƒ','Ï„','Ï…','Ï†','Ï‡','Ïˆ','Ï‰','ÏŒ','Ï','ÏŽ','Ð°','Ð±','Ð²','Ð³','Ð´','Ðµ','Ð¶','Ð·','Ð¸','Ð¹','Ðº','Ð»','Ð¼','Ð½','Ð¾','Ð¿','Ñ€','Ñ','Ñ‚','Ñƒ','Ñ…','Ñ†','Ñ‡','Ñˆ','Ñ‹','ÑŒ','Ñ','ÑŽ','Ñ','Ñ‘','ØŒ','Ø¢','Ø¦','Ø§','Ø¨','Øª','Ø¬','Ø­','Ø®','Ø¯','Ø±','Ø²','Ø³','Ø´','Øµ','Ø·','Ø¹','Øº','Ù','Ùƒ','Ù„','Ù…','Ù†','ÙˆÙ','Ù¹','Ù¾','Ú†','Ú‘','Ú©','Ú¯','Úº','Ú¾','Û','ÛŒ','Û’','Û”à¤à¤‚','à¤…','à¤†','à¤‡','à¤ˆ','à¤‰','à¤','à¤','à¤“','à¤”','à¤•','à¤–','à¤—','à¤˜','à¤š','à¤œ','à¤','à¤Ÿ','à¤ ','à¤¡','à¤£','à¤¤','à¤¥','à¤¦','à¤§','à¤¨','à¤ª','à¤«','à¤¬','à¤­','à¤®','à¤¯','à¤°','à¤²','à¤µ','à¤¶','à¤·','à¤¸','à¤¹à¤¼à¤¾à¤¿à¥€à¥à¥‚à¥ƒà¥‡à¥ˆà¥‰à¥‹à¥Œà¥','à¥š','à¥›','à¥œ','à¥','à¥ž','à¥¤à¦à¦‚','à¦…','à¦†','à¦‡','à¦‰','à¦','à¦“','à¦•','à¦–','à¦—','à¦™','à¦š','à¦›','à¦œ','à¦','à¦ž','à¦Ÿ','à¦¡','à¦£','à¦¤','à¦¥','à¦¦','à¦§','à¦¨','à¦ª','à¦«','à¦¬','à¦­','à¦®','à¦¯','à¦°','à¦²','à¦¶','à¦¸','à¦¹à¦¼à¦¾à¦¿à§€à§à§‚à§‡à§‹à§Œà§','à§Žà¬à¬‚','à¬…','à¬†','à¬‰','à¬','à¬•','à¬—','à¬š','à¬›','à¬œ','à¬Ÿ','à¬ ','à¬¡','à¬£','à¬¤','à¬¥','à¬¦','à¬§','à¬¨','à¬ª','à¬¬','à¬­','à¬®','à¬¯','à¬°','à¬³','à¬¶','à¬¸','à¬¹à¬¾à¬¿à­€à­à­‡à­‹à­','à­Ÿ','à­±','áº¡','áº¥','áº§','áº­','áº¯','áº±','áº»','áº¿','á»‰','á»','á»','á»‘','á»“','á»™','á»›','á»','á»Ÿ','â€‰','â€‹','â€Œ','â€“','â€”','â€•','â€œ','â€','â€ž','â€¢','â‚¬','â„¢','âˆ’','â”€','â”‚','â–Œ','â–','â–²','â—†','â—‹','â—','â˜…','â˜†','â˜»','â™¥','ã€‚','ã€Š','ã€‹','ã€','ã€‘','ä¸€','ä¸ƒ','ä¸‰','ä¸Š','ä¸‹','ä¸','ä¸Ž','ä¸–','ä¸œ','ä¸¤','ä¸§','ä¸­','ä¸°','ä¸¹','ä¸º','ä¹','ä¹˜','ä¹','ä¹¦','äº‰','äºŒ','äºŽ','äº‘','äº”','äº¤','äº«','äº¬','äº²','äºº','ä»Š','ä»–','ä»™','ä»£','ä»¬','ä¼˜','ä¼š','ä¼ ','ä¼¤','ä½†','ä½','ä½“','ä½™','ä½œ','ä½ ','ä¿','å„¿','å…ˆ','å…‰','å…¥','å…¨','å…«','å…±','å†…','å†™','å†¶','å†·','å‡ ','å‡º','åˆ†','åˆ‡','åˆ™','åˆ›','åˆ','åˆ«','åˆ»','å‰','å‰‘','åŠ›','åŠ¨','åŒ–','åŒ—','åŒº','å','åƒ','åˆ','åŠ','åŽ','å°','å·','åŽ„','åŽ»','åŽ¿','å‚','å¤','å¥','å¶','å·','åˆ','åŒ','åŽ','å¼','å’†','å’Œ','å’¬','å’½','å“€','å“','å“','å“­','å“®','å“½','å”','å–˜','å–·','åš¼','å››','å›ž','å›­','å›½','å›¾','åœ¨','åœ°','åŸŽ','å ª','å¢ƒ','å¢¨','å£«','å£°','å¤„','å¤•','å¤–','å¤š','å¤œ','å¤§','å¤©','å¤ª','å¤«','å¤±','å¤¹','å¥³','å¦ˆ','å§š','å¨¥','å­','å­—','å­™','å­Ÿ','å®‰','å®‹','å®˜','å®š','å®¢','å®£','å®µ','å®¹','å¯‚','å¯Œ','å¯¹','å¯º','å¯¼','å¯¿','å°‘','å°”','å°˜','å°±','å±±','å²','å²¸','å·±','å·²','å¸‚','å¸ˆ','å¸Œ','å¸­','å¸¸','å¹³','å¹´','å¹¿','åºœ','åº§','å¼€','å¼‹','å½¢','å½±','å¾…','å¾‹','å¾—','å¾ª','å¿ƒ','å¿†','å¿—','å¿«','æ€€','æ€’','æ€','æ©','æ¯','æ¸','æ‚ ','æ‚²','æƒ…','æƒœ','æ„','æ„¿','æ…ˆ','æˆ','æˆ‘','æˆ–','æˆ˜','æ‰','æ‰­','æ‰¿','æŠ–','æ‹‰','æŒ¥','æŒ½','æ','æ‘§','æ‘¸','æ’¼','æ”¹','æ•…','æ•™','æ•°','æ–‡','æ–°','æ–¹','æ–½','æ— ','æ—¥','æ—¦','æ—©','æ—¶','æ—·','æ˜Ÿ','æ˜¥','æ˜¯','æ™¯','æš–','æš´','æ›²','æ›´','æ›¾','æœˆ','æœ‰','æœ','æœª','æœµ','æœº','æœ½','æŽ','æŸ','æ¡','æ¥','æ¿','æž„','æž—','æž','æž¯','æŸ”','æŸ³','æ ‘','æ –','æ ¼','æ¡ƒ','æ¡','æ¢¦','æ¢¨','æ¬²','æ­Œ','æ­£','æ®‹','æ®¡','æ¯','æ¯”','æ¯•','æ¯«','æ°','æ°´','æ±‚','æ±‰','æ±Ÿ','æ± ','æ±©','æ²‰','æ²§','æ²³','æ³Š','æ³•','æ³£','æ³¨','æ³ª','æ³³','æ³¼','æ´‹','æ´ž','æ´¥','æ´»','æµ','æµ‘','æµ“','æµ·','æ¶Œ','æ·Œ','æ·™','æ·±','æ·¹','æ¸©','æ¸¸','æ¹–','æº','æ»¡','æ½º','ç¯','çµ','ç‚‰','ç‚¹','çƒŸ','ç„¶','ç…§','ç†”','ç† ','çˆ±','ç‰¡','ç‹‚','çŽ‰','ç€','ç¢','ç“¦','ç“¶','ç”Ÿ','ç”±','ç”·','ç”»','ç•Œ','ç–','ç—›','ç™½','ç™¾','çš„','çš†','ç›˜','ç››','ç›®','ç›¸','çœ','çœ‹','çœ¼','ç›','çŸ¥','çŸ«','ç£¬','ç¥','ç¥ˆ','ç¥ž','ç¥­','ç§ƒ','ç§‹','ç§','ç¨¿','ç©´','ç©º','ç©¿','çª—','ç« ','ç«¯','ç¬”','ç¬¬','ç­‰','ç®€','ç²¾','ç´«','ç¹','çº¸','ç»','ç»','ç¼˜','ç¼ ','ç½—','ç¾Ž','ç¿ ','ç¿»','è”','è‚²','èƒ†','è„‰','è‡£','è‡ª','è‡³','èˆŒ','è‰²','èŠ’','èŠ±','è‹¥','è‹±','è‰','è£','è·','èŽ±','èŽ²','èŠ','èŽ','è¦','è½','è‘—','è˜¸','è™š','è›‡','è¡€','è¡Œ','è£´','è£¸','è¥„','è¥¿','è§','è§„','è§…','è§¦','è¨€','è®¤','è®°','è®º','è¯','è¯‘','è¯—','è°¦','è°·','è±¡','èµ‹','èµ','èµ°','èµ·','è½®','è½½','è¾ˆ','è¾¾','è¿‡','è¿','è¿™','è¿›','è¿œ','è¿ž','è¿Ÿ','é€','é€š','é€','é€Ÿ','éµ','é‚€','é…’','é…¬','é…¸','é†‰','é†’','é‡Œ','é‡Ž','é’Ÿ','é“¶','é“º','é”','é”‹','é”™','é•œ','é•¿','é—¨','é—´','é—»','é˜³','é˜´','é˜µ','é™†','é™¶','é›„','é›…','é›•','é›ª','é›¾','éœœ','éœ­','éœ²','é’','é™','éŸ§','éŸ³','éŸµ','é¢˜','é¢¤','é£Ž','é£˜','é£™','é¥±','é¦–','é¦¨','é©¬','é­‚','é­„','é»˜','é¾™','ïŠ','ï¼','ï¼ˆ','ï¼‰','ï¼Œ','ï¼š','ï¼Ÿ','ðŸ’','ðŸ’','ðŸ’“'
            ]
            valid = False
            for char in chars_remove:
                if char in file:
                    purge = purge + 1
                    break
                else:
                    valid = True

            if valid == True:
                chars_remove = '[' + ''.join(chars_remove) + ']'
                file = re.sub(chars_remove, ' ', file)
                file = re.sub(' +', ' ', file).strip()
                poem_data.append({'file': poem_file_path,
                            'corpus': file})
    return poem_data

"""
cleans data from unnecessary stuff 
"""
def clean_data(corpus):
    chars = [len(x['corpus']) for x in corpus]
    idx = [x > 260 for x in chars]
    corpus = list(np.array(corpus)[np.array(idx)])

    lines = [len(x['corpus'].split('\n')) for x in corpus]
    idx = [((x > 8) & (x < 50)) for x in lines]
    corpus = list(np.array(corpus)[np.array(idx)])

    words = [len(re.findall(r'\w+', x['corpus'])) for x in corpus]
    idx = [((x > 30) & (x < 200)) for x in words]
    corpus = list(np.array(corpus)[np.array(idx)])

    words = [re.findall(r'\w+', x['corpus']) for x in corpus]
    words = [item for sublist in words for item in sublist]

    words_counter = nltk.FreqDist(words)
    words_counter = pd.DataFrame(words_counter.most_common())
    words_counter.columns = ['words', 'freq']
    words_counter['len'] = [len(x) for x in words_counter['words']]

    all_text = str()
    for x in corpus:
        all_text = all_text + x['corpus']

    chars_remove = ['\t', '"', '%', '&', '\*', '\+', '/', '0', '1', '2', '3', '4', 'ä¾„', 'Â¦'
                    '5', '6', '7', '8', '9', '=', '\[', '\]', '_',  '`', '{',
                    'Â¨', 'Âª', 'Â«', 'Â²', 'Âº', 'Â»', 'Ã ', 'Ã¢', 'Ã£', 'Ã¤', 'Ã§',
                    'Ã¨', 'Ãª', 'Ã¬', 'Ã¯', 'Ã²', 'Ã´', 'Ãµ', 'Ã¶', 'Ã¼', '\xa0', 'Â°', 
                    '~','Â¤','Â©','Â®','Â·','Ã¡','Ã¦','Ã©','Ã«','Ã­','Ã®','Ã°','Ã±','Ã³','Ã¸','Ã¹','Ãº','Ã»','Ã½','Ã¾','Ä','Äƒ','Ä‡','Ä','Ä‘','Ä«','Å“','Å›','ÅŸ','Å¡','Å£','Å©','Å«','Å¾','Æ¡','Æ°','È™','È›Ì€ÌÌƒÌ‰Ì£','Î¬','Î­','Î®','Î¯','Î±','Î²','Î³','Î´','Îµ','Î¶','Î·','Î¸','Î¹','Îº','Î»','Î¼','Î½','Î¾','Î¿','Ï€','Ï','Ï‚','Ïƒ','Ï„','Ï…','Ï†','Ï‡','Ïˆ','Ï‰','ÏŒ','Ï','ÏŽ','Ð°','Ð±','Ð²','Ð³','Ð´','Ðµ','Ð¶','Ð·','Ð¸','Ð¹','Ðº','Ð»','Ð¼','Ð½','Ð¾','Ð¿','Ñ€','Ñ','Ñ‚','Ñƒ','Ñ…','Ñ†','Ñ‡','Ñˆ','Ñ‹','ÑŒ','Ñ','ÑŽ','Ñ','Ñ‘','ØŒ','Ø¢','Ø¦','Ø§','Ø¨','Øª','Ø¬','Ø­','Ø®','Ø¯','Ø±','Ø²','Ø³','Ø´','Øµ','Ø·','Ø¹','Øº','Ù','Ùƒ','Ù„','Ù…','Ù†','ÙˆÙ','Ù¹','Ù¾','Ú†','Ú‘','Ú©','Ú¯','Úº','Ú¾','Û','ÛŒ','Û’','Û”à¤à¤‚','à¤…','à¤†','à¤‡','à¤ˆ','à¤‰','à¤','à¤','à¤“','à¤”','à¤•','à¤–','à¤—','à¤˜','à¤š','à¤œ','à¤','à¤Ÿ','à¤ ','à¤¡','à¤£','à¤¤','à¤¥','à¤¦','à¤§','à¤¨','à¤ª','à¤«','à¤¬','à¤­','à¤®','à¤¯','à¤°','à¤²','à¤µ','à¤¶','à¤·','à¤¸','à¤¹à¤¼à¤¾à¤¿à¥€à¥à¥‚à¥ƒà¥‡à¥ˆà¥‰à¥‹à¥Œà¥','à¥š','à¥›','à¥œ','à¥','à¥ž','à¥¤à¦à¦‚','à¦…','à¦†','à¦‡','à¦‰','à¦','à¦“','à¦•','à¦–','à¦—','à¦™','à¦š','à¦›','à¦œ','à¦','à¦ž','à¦Ÿ','à¦¡','à¦£','à¦¤','à¦¥','à¦¦','à¦§','à¦¨','à¦ª','à¦«','à¦¬','à¦­','à¦®','à¦¯','à¦°','à¦²','à¦¶','à¦¸','à¦¹à¦¼à¦¾à¦¿à§€à§à§‚à§‡à§‹à§Œà§','à§Žà¬à¬‚','à¬…','à¬†','à¬‰','à¬','à¬•','à¬—','à¬š','à¬›','à¬œ','à¬Ÿ','à¬ ','à¬¡','à¬£','à¬¤','à¬¥','à¬¦','à¬§','à¬¨','à¬ª','à¬¬','à¬­','à¬®','à¬¯','à¬°','à¬³','à¬¶','à¬¸','à¬¹à¬¾à¬¿à­€à­à­‡à­‹à­','à­Ÿ','à­±','áº¡','áº¥','áº§','áº­','áº¯','áº±','áº»','áº¿','á»‰','á»','á»','á»‘','á»“','á»™','á»›','á»','á»Ÿ','â€‰','â€‹','â€Œ','â€“','â€”','â€•','â€œ','â€','â€ž','â€¢','â‚¬','â„¢','âˆ’','â”€','â”‚','â–Œ','â–','â–²','â—†','â—‹','â—','â˜…','â˜†','â˜»','â™¥','ã€‚','ã€Š','ã€‹','ã€','ã€‘','ä¸€','ä¸ƒ','ä¸‰','ä¸Š','ä¸‹','ä¸','ä¸Ž','ä¸–','ä¸œ','ä¸¤','ä¸§','ä¸­','ä¸°','ä¸¹','ä¸º','ä¹','ä¹˜','ä¹','ä¹¦','äº‰','äºŒ','äºŽ','äº‘','äº”','äº¤','äº«','äº¬','äº²','äºº','ä»Š','ä»–','ä»™','ä»£','ä»¬','ä¼˜','ä¼š','ä¼ ','ä¼¤','ä½†','ä½','ä½“','ä½™','ä½œ','ä½ ','ä¿','å„¿','å…ˆ','å…‰','å…¥','å…¨','å…«','å…±','å†…','å†™','å†¶','å†·','å‡ ','å‡º','åˆ†','åˆ‡','åˆ™','åˆ›','åˆ','åˆ«','åˆ»','å‰','å‰‘','åŠ›','åŠ¨','åŒ–','åŒ—','åŒº','å','åƒ','åˆ','åŠ','åŽ','å°','å·','åŽ„','åŽ»','åŽ¿','å‚','å¤','å¥','å¶','å·','åˆ','åŒ','åŽ','å¼','å’†','å’Œ','å’¬','å’½','å“€','å“','å“','å“­','å“®','å“½','å”','å–˜','å–·','åš¼','å››','å›ž','å›­','å›½','å›¾','åœ¨','åœ°','åŸŽ','å ª','å¢ƒ','å¢¨','å£«','å£°','å¤„','å¤•','å¤–','å¤š','å¤œ','å¤§','å¤©','å¤ª','å¤«','å¤±','å¤¹','å¥³','å¦ˆ','å§š','å¨¥','å­','å­—','å­™','å­Ÿ','å®‰','å®‹','å®˜','å®š','å®¢','å®£','å®µ','å®¹','å¯‚','å¯Œ','å¯¹','å¯º','å¯¼','å¯¿','å°‘','å°”','å°˜','å°±','å±±','å²','å²¸','å·±','å·²','å¸‚','å¸ˆ','å¸Œ','å¸­','å¸¸','å¹³','å¹´','å¹¿','åºœ','åº§','å¼€','å¼‹','å½¢','å½±','å¾…','å¾‹','å¾—','å¾ª','å¿ƒ','å¿†','å¿—','å¿«','æ€€','æ€’','æ€','æ©','æ¯','æ¸','æ‚ ','æ‚²','æƒ…','æƒœ','æ„','æ„¿','æ…ˆ','æˆ','æˆ‘','æˆ–','æˆ˜','æ‰','æ‰­','æ‰¿','æŠ–','æ‹‰','æŒ¥','æŒ½','æ','æ‘§','æ‘¸','æ’¼','æ”¹','æ•…','æ•™','æ•°','æ–‡','æ–°','æ–¹','æ–½','æ— ','æ—¥','æ—¦','æ—©','æ—¶','æ—·','æ˜Ÿ','æ˜¥','æ˜¯','æ™¯','æš–','æš´','æ›²','æ›´','æ›¾','æœˆ','æœ‰','æœ','æœª','æœµ','æœº','æœ½','æŽ','æŸ','æ¡','æ¥','æ¿','æž„','æž—','æž','æž¯','æŸ”','æŸ³','æ ‘','æ –','æ ¼','æ¡ƒ','æ¡','æ¢¦','æ¢¨','æ¬²','æ­Œ','æ­£','æ®‹','æ®¡','æ¯','æ¯”','æ¯•','æ¯«','æ°','æ°´','æ±‚','æ±‰','æ±Ÿ','æ± ','æ±©','æ²‰','æ²§','æ²³','æ³Š','æ³•','æ³£','æ³¨','æ³ª','æ³³','æ³¼','æ´‹','æ´ž','æ´¥','æ´»','æµ','æµ‘','æµ“','æµ·','æ¶Œ','æ·Œ','æ·™','æ·±','æ·¹','æ¸©','æ¸¸','æ¹–','æº','æ»¡','æ½º','ç¯','çµ','ç‚‰','ç‚¹','çƒŸ','ç„¶','ç…§','ç†”','ç† ','çˆ±','ç‰¡','ç‹‚','çŽ‰','ç€','ç¢','ç“¦','ç“¶','ç”Ÿ','ç”±','ç”·','ç”»','ç•Œ','ç–','ç—›','ç™½','ç™¾','çš„','çš†','ç›˜','ç››','ç›®','ç›¸','çœ','çœ‹','çœ¼','ç›','çŸ¥','çŸ«','ç£¬','ç¥','ç¥ˆ','ç¥ž','ç¥­','ç§ƒ','ç§‹','ç§','ç¨¿','ç©´','ç©º','ç©¿','çª—','ç« ','ç«¯','ç¬”','ç¬¬','ç­‰','ç®€','ç²¾','ç´«','ç¹','çº¸','ç»','ç»','ç¼˜','ç¼ ','ç½—','ç¾Ž','ç¿ ','ç¿»','è”','è‚²','èƒ†','è„‰','è‡£','è‡ª','è‡³','èˆŒ','è‰²','èŠ’','èŠ±','è‹¥','è‹±','è‰','è£','è·','èŽ±','èŽ²','èŠ','èŽ','è¦','è½','è‘—','è˜¸','è™š','è›‡','è¡€','è¡Œ','è£´','è£¸','è¥„','è¥¿','è§','è§„','è§…','è§¦','è¨€','è®¤','è®°','è®º','è¯','è¯‘','è¯—','è°¦','è°·','è±¡','èµ‹','èµ','èµ°','èµ·','è½®','è½½','è¾ˆ','è¾¾','è¿‡','è¿','è¿™','è¿›','è¿œ','è¿ž','è¿Ÿ','é€','é€š','é€','é€Ÿ','éµ','é‚€','é…’','é…¬','é…¸','é†‰','é†’','é‡Œ','é‡Ž','é’Ÿ','é“¶','é“º','é”','é”‹','é”™','é•œ','é•¿','é—¨','é—´','é—»','é˜³','é˜´','é˜µ','é™†','é™¶','é›„','é›…','é›•','é›ª','é›¾','éœœ','éœ­','éœ²','é’','é™','éŸ§','éŸ³','éŸµ','é¢˜','é¢¤','é£Ž','é£˜','é£™','é¥±','é¦–','é¦¨','é©¬','é­‚','é­„','é»˜','é¾™','ïŠ','ï¼','ï¼ˆ','ï¼‰','ï¼Œ','ï¼š','ï¼Ÿ','ðŸ’','ðŸ’','ðŸ’“'
    ]

    chars_remove = '[' + ''.join(chars_remove) + ']'
    # erase from docs non meaningful characters
    for doc in corpus:
        doc['corpus'] = re.sub(chars_remove, ' ', doc['corpus'])
        doc['corpus'] = re.sub(' +', ' ', doc['corpus']).strip()

    # line space: '\r\n ' '\r\n' to '\n', '\r\r\n'
    for doc in corpus:
        for pattern in ['\r\r\n', '\r\n ', '\r\n', '\n\n', '\r']:
            doc['corpus'] = re.sub(pattern, '\n', doc['corpus'])

    # Model learns when to start/end
    for doc in corpus:
        doc['corpus'] = doc['corpus'] + '.$'

    return corpus

"""
maps characters in a dictionary
"""
def get_vocabulary(corpus):

    all_text = str()
    for x in corpus:
        all_text = all_text + x['corpus']
  
    characters = sorted(list(set(all_text)))
    
    n_to_char = {n:char for n, char in enumerate(characters)}
    char_to_n = {char:n for n, char in enumerate(characters)}
    
    return characters, n_to_char, char_to_n  

"""
split, train, and test
"""
def corpus_split(corpus, split):
    idx = [i for i in range(len(corpus))]
    idx_train = np.random.choice(idx, size=int(len(corpus)*split), replace=False)
    idx_test = [i for i in idx if i not in idx_train]
    corpus_train = [corpus[i] for i in idx_train]
    corpus_test = [corpus[i] for i in idx_test]    
    return corpus_train, corpus_test

# Create tensor data from corpus
def build_data(corpus, char_to_n, max_seq = 100, stride = [1,6]):
    data_x = []
    data_y = []
    sequences = []
    max_seq+=1  

    for i in range(len(corpus)):
        text = corpus[i]['corpus']
        text_length = len(text)
        j = max_seq
        while j < text_length + stride[0]:
            k_to = min(j, text_length) # 
            k_from = (k_to - max_seq)            
            sequence = text[k_from:k_to] 
            sequence_encoded = np.array([char_to_n[x] for x in sequence]) 
            sequences.append(sequence)
            data_x.append(sequence_encoded[:-1])
            data_y.append(sequence_encoded[1:])
            j+=int(np.random.uniform(stride[0], stride[1]+1))       
    data_x = np.array(data_x) 
    data_y = np.array(data_y) 
    data_x, data_y = shuffle(data_x, data_y)
    return data_x, data_y

def get_tensor_data(corpus_train, corpus_test, char_to_n, max_seq, stride ):
  train_x, train_y = build_data(corpus_train, char_to_n, 
                                max_seq = max_seq, stride=stride)
  if len(corpus_test):
      test_x, test_y = build_data(corpus_test, char_to_n, 
                                    max_seq = max_seq, stride=stride)
  else:
      test_x, test_y = None, None
  
  return train_x, train_y, test_x, test_y

def get_data():
    SPLIT = 1
    MAX_SEQ = 120
    STRIDE = [MAX_SEQ/2, MAX_SEQ] 

    poem_dir_path = '../data/poetry_data'
    poem_corpus = read_poems(poem_dir_path)

    corpus = clean_data(poem_corpus)

    characters, n_to_char, char_to_n =  get_vocabulary(corpus)

    words_mapping = {'characters': characters,
                    'n_to_char': n_to_char,
                    'char_to_n': char_to_n}

    corpus_train, corpus_test = corpus_split(corpus, split=SPLIT)

    train_x, train_y, test_x, test_y = get_tensor_data(corpus_train=corpus_train, corpus_test=corpus_test, char_to_n = char_to_n, max_seq=MAX_SEQ, stride=STRIDE)

    data = {'corpus': corpus,
            'words_mapping': words_mapping,
            'train_x': train_x ,
            'train_y': train_y,
            'test_x' : test_x,
            'test_y' : test_y}

    # save file
    with open('../data/processed/processed_poems.pickle', 'wb') as file:
        pickle.dump(data, file)
    print('Data saved in:', 'data/processed/processed_poems.pickle')

    return corpus_train, corpus_test